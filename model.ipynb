{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw-psJQln-yY"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"daspartho/anime-or-not\", split='train') # i've uploaded the dataset on HuggingFace Datasets :)\n",
        "ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
        "ds"
      ],
      "metadata": {
        "id": "0IKJv8_OoEk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok_func(x): return tokenizer(x[\"plot\"], padding=True, truncation=True)\n",
        "\n",
        "tok_ds = ds.map(tok_func, batched=True)\n",
        "tok_ds"
      ],
      "metadata": {
        "id": "4GFj0aCIoGBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "bs = 128\n",
        "epochs = 15\n",
        "lr = 1e-5\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "args = TrainingArguments('model', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True, \n",
        "                         evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, \n",
        "                         num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "trainer = Trainer(model, args, train_dataset=tok_ds['train'], eval_dataset=tok_ds['test'], \n",
        "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "trainer.train();"
      ],
      "metadata": {
        "id": "x5Gd3td9oJOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "u9r6XKy4cNrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}